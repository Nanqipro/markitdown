IEEE TRANSACTIONS ON MEDICAL IMAGING (UNDER REVISION)

1

BrainGB: A Benchmark for Brain Network
Analysis with Graph Neural Networks

Hejie Cui, Wei Dai, Yanqiao Zhu, Xuan Kan, Antonio Aodong Chen Gu
Joshua Lukemire, Liang Zhan, Lifang He, Ying Guo, Carl Yang

2
2
0
2

t
c
O
5
1

]

C
N
.
o
i
b
-
q
[

2
v
4
5
0
7
0
.
4
0
2
2
:
v
i
X
r
a

Abstract— Mapping the connectome of the human brain
using structural or functional connectivity has become one
of the most pervasive paradigms for neuroimaging analysis.
Recently, Graph Neural Networks (GNNs) motivated from
geometric deep learning have attracted broad interest due
to their established power for modeling complex networked
data. Despite their superior performance in many ﬁelds,
there has not yet been a systematic study of how to design
effective GNNs for brain network analysis. To bridge this
gap, we present BrainGB, a benchmark for brain network
analysis with GNNs. BrainGB standardizes the process by
(1) summarizing brain network construction pipelines for
both functional and structural neuroimaging modalities and
(2) modularizing the implementation of GNN designs. We
conduct extensive experiments on datasets across cohorts
and modalities and recommend a set of general recipes for
effective GNN designs on brain networks. To support open
and reproducible research on GNN-based brain network
analysis, we host the BrainGB website at https://brai
ngb.us with models, tutorials, examples, as well as an
out-of-box Python package. We hope that this work will
provide useful empirical evidence and offer insights for
future research in this novel and promising direction.

Index Terms— Brain network analysis, graph neural net-
works, geometric deep learning for neuroimaging, datasets,
benchmarks

I. INTRODUCTION

H UMAN brains are at the center of complex neurobiolog-

ical systems in which neurons, circuits, and subsystems
interact to orchestrate behavior and cognition. Understanding
the structures, functions, and mechanisms of human brains has
been an intriguing pursuit for researchers with various goals,
including neural system simulation, mental disorder therapy,
as well as general artiﬁcial intelligence. Recent studies in
neuroscience and brain imaging have reached the consensus
that interactions between brain regions are key driving factors
for neural development and disorder analysis [1, 2]. Inspired
by graph theory, brain networks composed of nodes and edges
are developed to describe the interactions among brain regions.

H. Cui, W. Dai, X. Kan, A. C. Gu, and C. Yang are with the Department

of Computer Science, Emory University.

Y. Zhu is with the Department of Computer Science, University of

California, Los Angeles.

J. Lukemire and Y. Guo are with the Department of Biostatistics and

Bioinformatics, Emory University.

L. Zhan is with the Department of Electrical and Computer Engineering,

University of Pittsburgh.

L. He is with the Department of Computer Science and Engineering,

Lehigh University.

Correspondence should be addressed to C. Yang (e-mail:

j.carlyang@emory.edu).

The human brain can be scanned through various medical
imaging techniques, including Magnetic-Resonance Imaging
(MRI), Electrogastrography (EEG), Positron Emission Tomog-
raphy (PET), and so on. Among all these acquisitions, MRI
data are the most widely used for brain analysis research. There
are also different modalities of MRI data such as functional
MRI (fMRI) and Diffusion Tensor Imaging (DTI), from which
functional and structural brain networks can be constructed
respectively. Speciﬁcally, the connectivity in functional brain
networks describes correlations between time-series signals
of brain regions, while the connectivity in structural brain
networks models the physical connectivity between gray matter
regions [3]. Both functional and structural connections are
widely acknowledged as valuable resources of information for
brain investigation [4, 5].

Previous work on brain network analysis has studied
shallow models based on graph theory [5, 6] and tensor
factorization [7, 8] extensively, which focuses on proposing
neurobiologically insightful graph measures and approaches
from the node, motif, and graph level to detect network
communities or modules and identify central network elements.
Methodological developments in graph research enable us to
quantify more topological characteristics of complex systems,
many of which have already been assessed in brain networks,
such as modularity, hierarchy, centrality, and the distribution of
network hubs. However, shallow modeling techniques can be
inadequate for the sophisticated connectome structures of brain
networks [9]. On the other hand, deep learning models have
become extraordinarily popular in machine learning, achieving
impressive performance on images [10, 11], videos [12],
and speech processing tasks [13]. These regular data are
represented in 1D/2D/3D Euclidean spaces and can be suitably
handled by traditional Recurrent (RNNs) or Convolutional
Neural Networks (CNNs). In contrast, the irregular structural
and functional brain connectivity networks constructed from
neuroimaging data are more complex due to their non-Euclidean
characteristics. In recent years, Graph Neural Networks (GNNs)
have attracted broad interest due to their established power for
analyzing graph-structured data [14–16]. Several pioneering
deep models have been devised to predict brain diseases by
learning graph structures of brain networks. For instance, Li
et al. [1] propose BrainGNN to analyze fMRI data, where ROI-
aware graph convolutional layers and ROI-selection pooling
layers are designed for neurological biomarker prediction.
Kawahara et al. [17] design a CNN framework BrainNetCNN
composed of edge-to-edge, edge-to-node, and node-to-graph
convolutional ﬁlters that leverage the topological locality of
structural brain networks. However, they mainly experiment

2

IEEE TRANSACTIONS ON MEDICAL IMAGING (UNDER REVISION)

Fig. 1: An overview of our BrainGB framework for brain network analysis with graph neural networks.

with their proposed models on speciﬁc private datasets. Due to
the ethical issue of human-related research, the datasets used
are usually not publicly available and the details of imaging
preprocessing are not disclosed, rendering the experiments
irreproducible for other researchers.

To address the aforementioned limitations,

there is an
urgent need for a public benchmark platform to evaluate
deep graph models for brain network analysis. However, it is
non-trivial to integrate different components within a uniﬁed
benchmarking platform. Current brain network analyses are
typically composed of two steps. The ﬁrst step is to construct
brain networks from neuroimaging data. Then, in the second
stage, the resulting brain connectivity between all node pairs is
used to classify individuals or predict clinical outcomes. The
difﬁculties in the initial stage are mostly due to restricted data
accessibility and sophisticated brain imaging preprocessing and
network construction pipelines that differ across cohorts and
modalities. The difﬁculty of the second stage is to establish a
standard evaluation pipeline based on fair experimental settings,
metrics, and modular-designed baselines that can be easily
validated and extended for future research.

In this work, we propose Brain Graph Neural Network
Benchmark (BrainGB)—a novel attempt to benchmark brain
network analysis with GNNs to the best of our knowledge.
The overview of BrainGB is demonstrated in Fig. 1 and the
main contributions are four-fold:

• A uniﬁed, modular, scalable, and reproducible framework
is established for brain network analysis with GNNs to
facilitate reproducibility. It is designed to enable fair
evaluation with accessible datasets, standard settings, and
baselines to foster a collaborative environment within com-
putational neuroscience and other related communities.
• We summarize the preprocessing and construction
pipelines for both functional and structural brain networks
to bridge the gap between the neuroimaging and deep
learning community.

• We decompose the design space of interest for GNN-
based brain network analysis into four modules: (1)
node features, (b) message passing mechanisms, (c)
attention mechanisms, and (d) pooling strategies. Different
combinations based on these four dimensions are provided

as baselines, and the framework can be easily extended
to new variants.

• We conduct a variety of empirical studies and suggest
a set of general recipes for effective GNN designs on
brain networks, which could be a starting point for further
studies.

To foster future research, we release the source code of
BrainGB at https://github.com/HennyJie/Brai
nGB and provide an out-of-box package that can be installed
directly, with detailed tutorials available on our hosted website
at https://braingb.us. Preprocessing instructions
and models are provided for standardized model evaluations.
We enable the community to collaboratively contribute by
submitting their own custom models, and we will maintain a
leaderboard to ensure such efforts will be recorded.

II. PRELIMINARIES

A. Brain Network Analysis

Brain networks are complex graphs with anatomic Regions
of Interest (ROIs) represented as nodes and connectivities
between the ROIs as links [18]. In recent years, the analysis
of brain networks has become increasingly important
in
neuroimaging studies to understand human brain organization
across different groups of individuals [19–23]. Abundant
ﬁndings in neuroscience research suggest that neural circuits
are highly related to brain functions, with aberrations in these
neural circuits being identiﬁed in diseased individuals [24–26].
Formally, in the task of brain network analysis, the input
is a brain network dataset D = {Gn, yn}N
n=1 consisting of N
subjects, where Gn = {Vn, En} represents the brain network of
subject n and yn is the subject’s label of the prediction, such
as neural diseases. In D, the brain network Gn of every subject
n involves the same set of M nodes deﬁned by the ROIs
on a speciﬁc brain parcellation, i.e., ∀n, Vn = V = {vi}M
i=1.
The difference across subjects lies in the edge connections
En among M brain regions, which are often represented by
a weighted adjacency matrix Wn ∈ RM ×M describing the
connection strengths between ROIs. The edge weights in W
are real-valued and the edges are potentially dense and noisy.
The model outputs a prediction ˆyn for each subject n, which
can be further analyzed in terms of features and biomarkers.

StructuralBrainNetworkssMRIdMRITimefMRIClinicalOutcomesFunctionalBrainNetworksMessagePassingGraph Neural NetworksParkinson’sProgression,Gender,Autism spectrum disorder…PredictionBiomarkersxixjNodefeatureInput:Gn=(X,E)hihjMessagemijMessagepassinghihjMessagemijαijAttentionweightAttention-enhancedmessagepassingMPGNNMPGNNwithattentiongnGraphpoolingOutput:gnMultiplelayersCUI et al.: BRAINGB: BENCHMARK FOR BRAIN NETWORK ANALYSIS WITH GRAPH NEURAL NETWORKS

3

Given brain networks constructed from different modalities
such as Diffusion Tensor Imaging (DTI) and functional
Magnetic Resonance Imaging (fMRI) [5, 27, 28], effective
analysis of the neural connectivities of different label groups
(e.g., disease, gender) plays a pivotal role in understanding
the biological structures and functions of the complex neural
in the early diagnosis of
system, which can be helpful
neurological disorders and facilitate neuroscience research [29–
35]. Previous models on brain networks are mostly shallow,
such as graph kernels [36] and tensor factorization [37, 38],
which are unable to model the complex graph structures of the
brain networks [9].

B. Graph Neural Networks

Graph Neural Networks (GNNs) have revolutionized the
ﬁeld of graph modeling and analysis for real-world networked
data such as social networks [14], knowledge graphs [39],
protein or gene interaction networks [15], and recommendation
systems [40]. The advantage of GNNs is that they can combine
node features and graph structures in an end-to-end fashion as
needed for speciﬁc prediction tasks. A generic framework of
GNN could be represented in two phases. In the ﬁrst phase,
it computes the representation hi of each node vi ∈ Vn
by recursively aggregating messages from vi’s multi-hop
neighborhood, where h0
is initialized with node features.
i
After getting the last-layer node representation h(L), an extra
pooling strategy is adopted to obtain the graph representation.
Thereafter, a Multi-Layer Perceptron (MLP) can be applied to
make predictions on the downstream tasks.

It is worth noting that brain networks are different from other
real-world graphs such as social networks or knowledge graphs,
due to (1) the lack of useful initial node (ROI) features on brain
networks represented by featureless graphs, (2) the real-valued
connection weights that can be both positive or negative, and (3)
the ROI identities and their orders are ﬁxed across individual
graph samples within the same dataset. The design of GNN
models should be customized to ﬁt the unique nature of brain
network data. Recently, there have been emerging efforts on
GNN-based brain network analysis [1, 17, 41–47]. However,
these models are only tested on speciﬁc local datasets, mainly
due to the convention in neuroscience that researchers are more
used to developing methods that are applicable to their speciﬁc
datasets and the regulatory restrictions that most brain imaging
datasets are usually restrictively public, meaning that qualiﬁed
researchers need to request access to the raw imaging data
and preprocess them to obtain brain network data, but they are
not allowed to release the preprocessed data afterwards. These
challenges largely prohibit the methodology development in
computational neuroscience research.

III. BRAIN NETWORK DATASET CONSTRUCTION

A. Background: Diverse Modalities of Brain Imaging

Models of the human brain as a complex network have
attracted increasing attention due to their potential for helping
understand human cognition and neurological disorders. In
practice, human brain data can be acquired through various

scanning techniques [48], such as Magnetic-Resonance Imag-
ing (MRI), Electroencephalography (EEG) and Magnetoen-
cephalography (MEG), Positron Emission Tomography (PET),
Single-Photon Emission Computed Tomography (SPECT), and
X-ray Computed Tomography (CT). Among them, MRI is
one of the most widely used techniques in brain research
and clinical practice, due to its large range of available tissue
contrast, detailed anatomical visualization, and high sensitivity
to abnormalities [49].

1) MRI Data: In this paper, we focus on MRI-derived
brain networks. Speciﬁcally, for different modalities of MRI
data, we can reconstruct different types of brain networks.
Functional MRI (fMRI) is one of the most popular modalities
for investigating brain function and organization [31, 32, 50]
by detecting changes in blood oxygenation and blood ﬂow that
occur in response to neural activity. Diffusion-weighted MRI
(dMRI), on the other hand, can enable inference about the
underlying connection structure in the brain’s white matter by
recording the diffusion trajectory of molecules (usually water).
fMRI focuses on functional activity, while dMRI presents brain
structural information from different perspectives. Speciﬁcally,
two types of brain networks, functional and structural, can be
constructed from the aforementioned modalities by following
different connectivity generation paradigms [51].

2) Challenges in MRI Preprocessings: The raw MRI data
collected from scanners is not directly usable for brain network
construction or imaging analysis. A complicated preprocessing
pipeline is necessary to remove unwanted artifacts, transform
the data into a standard format, and perform structure discovery.
Although there are several widely-used neuroimaging data
preprocessing tools, such as SPM1, AFNI2 and FSL3, each
of them still needs considerable training and learning efforts.
Moreover, the functionality of these software varies, and for
dMRI, no one software contains all the necessary preprocessing
capabilities. In addition, many neuroimaging datasets cannot
be made public due to privacy or ethical concerns. Due to the
variety of preprocessing approaches and issues with making
data publically available, there are difﬁculties in reproducibility
in neuroimaging studies. Additionally, the preprocessing steps
are distinctive across modalities. All these challenges make it
difﬁcult for deep learning researchers with little knowledge in
medical imaging processing to get into the ﬁeld.

B. Brain Network Construction from Raw Data

In this section, we provide a general overview of the standard
preprocessing pipelines for the construction of brain networks
of different modalities. Due to the regulation restrictions for
direct sharing of the brain network data, we provide two
complete pipelines, one for functional brain networks (ABCD4
speciﬁcally) and one for structural brain networks (PPMI5
speciﬁcally), with step-by-step commands and parameter
settings on our hosted website for public access6. The idea

1https://www.fil.ion.ucl.ac.uk/spm/software/spm12
2https://afni.nimh.nih.gov
3https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL
4https://nda.nih.gov/abcd
5https://www.ppmi-info.org
6https://braingb.us/preprocessing

4

IEEE TRANSACTIONS ON MEDICAL IMAGING (UNDER REVISION)

Fig. 2: The framework of fMRI data preprocessing and functional brain network construction procedures, with recommended
tools for each step shown on the right. The more commonly-used tools for the functional modality are placed at the front.

Fig. 3: The framework of dMRI data preprocessing and structural brain network construction procedures, with recommended
tools for each step shown on the right. The more commonly-used tools for the structural modality are placed at the front.

is, by following our instructions there, researchers with basic
programming skills can easily construct similar brain networks
after requesting the data access by themselves, so everyone in
the community can compare their methods fairly on the same
datasets.

1) Functional Brain Network Construction: The left side
of Fig. 2 shows a standard preprocessing procedure for
functional brain imaging, with the corresponding commonly-
used toolboxes (i.e., SPM121, AFNI2, FSL3, FreeSurfer7,
CONN8, fMRI Prep9, ANTs10, Nilearn11) shown on the right
side. Note that each step in the preprocessing and network
construction pipeline needs quality control by the experts,
and the speciﬁc order of preprocessing steps may change
slightly based on the acquisition conditions of the dataset. Some
representative functional neuroimaging datasets in literature to

7https://surfer.nmr.mgh.harvard.edu
8https://web.conn-toolbox.org/home
9https://fmriprep.org/en/stable/index.html
10http://stnava.github.io/ANTs
11https://nilearn.github.io/stable/index.html

facilitate scientiﬁc research include ADHD 200 [52], ADNI
(fMRI part) [53], HCP 900 [54], ABIDE [55], etc.

To measure functional connectivity, some preprocessing of
the fMRI time series is often performed including detrending,
demeaning, and whitening fMRI BOLD time series at each
voxel [56]. To construct the brain networks, a brain atlas or a
set of Regions of Interest (ROI) are selected to deﬁne the nodes.
Then, the representative fMRI BOLD series from each node
are obtained by either averaging or performing Singular Value
Decomposition (SVD) on the time series from all the voxels
within the node. Various measures have been proposed for
assessing brain connectivity between pairs of nodes. One of the
simplest and most frequently used methods in the neuroimaging
community is via pairwise correlations between BOLD time
courses from two ROIs. Other methods including partial
correlations [56], mutual
information, coherence, Granger
causality, etc [57]. After selecting the Functional Connectivity
(FC) measure, one can evaluate the strength of connectivity
between each pair of nodes in the brain network. Often,
some transformation, such as the Fisher’s transformation, is

Brain ExtractionRemove unnecessary voxels such as bone, air, etc. from T1/T2, apply generated brain mask to fMRI dataSlice-Timing CorrectionAdjust for the fact that each slice in the volume is taken at a different time, not all at onceMotion Correction/RealignmentCorrect  movement made during scanning by aligning all the functional images with one referenceCo-registrationApply EPI distortion correction and align the functional images with the structural images for localizationNormalizationWarp the data across subjects to a template/atlas standardized spaceSmoothingPerform weighted averages of individual voxels with neighboring voxelsBrainRegionParcellationSegmenteachsubjectintotheROIdefinedbythegivenatlasConstruct NetworkCalculate pairwise correlations between ROIs as edgesSPM12AFNIFSLFreeSurferCONNfMRIPrepANTsNilearn✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓Recommended Software: CONN, GraphVar, Brain Connectivity ToolboxFunctionalBrainNetworkConstructionFunctional MRI Data PreprocessingFSLAFNIFreeSurferTrackVis3DSliderTortoiseMRtrix3DSI StudioDIPYTractoFlow✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓Eddy-currentandHeadMotionCorrectionAlignall rawimagestotheb0imagetocorrectforheadmotionandeddycurrentdistortionsEPIInducedSusceptibilityArtifactsCorrectionCorrectthespatiallynonlineardistortionscausedbyB0inhomogeneitiesinEcho-planarimagingBrain ExtractionRemove voxels not necessary for analysis such as bone, dura, air, etc., leaving just the brainReconstruct Local Diffusion PatternFitadiffusiontensormodelateachvoxelonpreprocessedandeddycurrentcorrecteddataTractographyReconstructbrainconnectivitygraphsusingwholebraintractographyalgorithmslikeFACTBrainRegionParcellationParcellateROIsfromT1-weightedstructuralMRI and map those ROIs to DTI spaceConstruct NetworkComputethenetworkbasedonthegeneratedlabelandthereconstructedwholebraintractographyRecommended Software: FSL, Metric, DSI StudioStructuralBrainNetworkConstructionDiffusion MRI Data PreprocessingCUI et al.: BRAINGB: BENCHMARK FOR BRAIN NETWORK ANALYSIS WITH GRAPH NEURAL NETWORKS

5

performed to transform the original FC measures to improve
their distribution properties. The transformed FC measures can
then be utilized for the subsequent analysis of functional brain
networks.

To facilitate public testing, we take Adolescent Brain
Cognitive Development Study (ABCD) as an example and
provide a step-by-step instruction for functional brain network
construction on our hosted BrainGB website12. The ABCD-
HCP BIDS13 pipeline is used to preprocess the data. In
brief, anatomical preprocessing included normalization, co-
registration, segmentation, and brain extraction. Functional data
preprocessing included slice-time correction, motion correction,
distortion correction, co-registration, normalization, and spatial
smoothing. Brain parcellation schemes were then applied to the
functional data to obtain time courses for each ROI, and Pearson
correlation was used to construct brain networks representing
the connectivity between ROIs.

2) Structural Brain Network Construction: Structural brain
networks provide a systematic perspective for studying the
anatomical and physiological organization of human brains
and help to understand how brain structure inﬂuences function.
Some representative neuroimaging studies include diffusion
MRI data are PPMI [58], ADNI [53], HCP [54], AIBL [59],
OASIS [59], etc. The commonly-used toolboxes for dMRI
include FSL3, AFNI2, FreeSurfer7, TrackVis14, 3D Slicer15,
Tortoise16, MRtrix317, DSI Studio18.

The left side of Fig. 3 summarizes the pipeline for recon-
structing the structural brain network. Preprocessing steps for
the dMRI data include removal of eddy current-induced distor-
tions, brain extraction, and co-registration between diffusion
and structural images. Next, some modeling strategies are
applied to reconstruct the local diffusion patterns. Commonly
adopted models include the DTI modeling, which ﬁts a
tensor model or muti-tensor model [60] to capture the local
diffusion patterns, and the Ball and Sticks model [61]. After
reconstructing the local diffusion patterns, a tractography
algorithm is performed to computationally reconstruct ﬁber tract
connections between brain regions. Commonly-used algorithms
include the deterministic tractography [62] and the probabilistic
tractography [63]. The deterministic tractography connects
neighboring voxels from seed regions based on the major
direction of the DTI tensor. The probabilistic tractography
involves ﬁrst estimating ﬁber orientation and its uncertainty
at each voxel and building a diffusion path probability map
based on the estimated orientation and uncertainty. While
deterministic tractography is a more computationally efﬁcient
approach to reconstruct major ﬁber bundles in the brain,
probabilistic tractography has become more popular because
it is more robust to noise and allows tractography to progress
beyond uncertain regions by taking into account uncertainty
in ﬁber orientations at each voxel [64]. To construct the

12https://braingb.us/preprocessing
13https://github.com/DCAN-Labs/abcd-hcp-pipeline
14http://trackvis.org
15https://www.slicer.org
16https://tortoise.nibib.nih.gov
17https://www.mrtrix.org
18https://dsi-studio.labsolver.org

structural network, the structure connectivity for each node
pair is calculated based on the empirical probability of ﬁber
tracts connecting the two regions. Note that each step in the
preprocessing and network construction needs quality control
from experts.

Similarly to functional brain network construction, we also
take Parkinson’s Progression Markers Initiative (PPMI) as
an example and provide an instruction pipeline for structural
brain network construction on our hosted BrainGB website19.
Speciﬁcally, the Diffusion Toolkit from TrackVis is used to
reconstruct local diffusion patterns and tractography. The brain
region parcellation is completed with both FSL and Freesurfer.
Then local diffusion pattern reconstruction and the network
computation are further performed by calculating the number
of ﬁbers within each ROI after removing the false positive
ones.

C. Discussions

In addition to the mainstream methods of constructing
connections in brain networks discussed above, there are also
other ways to construct different types of edges. For example,
directional connectivity that characterizes effective interactions
for fMRI [65]; hybrid functional brain networks where different
orders of relationships can be sensitive to different levels of
signal changes [66]; and dynamic functional brain networks
which include derivatives of windowed functional network
connectivity in the identiﬁcation of reoccurring states of
connectivity [65, 67]. Apart from fMRI and DTI, the most
commonly used modalities to construct functional and structural
brain networks, other neuroimaging modalities have also
been explored in literature, such as metabolic brain network
constructed from PET imaging [68], functional brain network
constructed from EEG signals [69], etc. Recent studies have
shown that the combination of both functional and structural
neuroimaging modalities can be more effective than using only
a single one, which can exploit complementary information
across different modalities [4, 70].

IV. GNN BASELINES FOR BRAIN NETWORK ANALYSIS

The process of applying GNNs to brain networks starts from
initialization of the ROI features, followed by the forward
pass which includes two phases, message passing, and pooling.
The learned graph-level representation then can be utilized for
brain disease analysis. In the machine learning domain, the
rapid evolution of GNNs has led to a growing number of new
architectures. Speciﬁcally for GNNs on brain network analysis,
we decompose the design space of interest for basic message
passing GNNs into four modules: node feature construction,
message passing, attention enhanced message passing, and
pooling strategies. An illustration of these modules is shown
in the middle of Fig. 1.

A. Node Feature Construction

In neuroscience analysis, researchers mostly focus on brain
connectivity represented by a featureless graph. To apply GNNs

19https://braingb.us/preprocessing

6

IEEE TRANSACTIONS ON MEDICAL IMAGING (UNDER REVISION)

on non-attributed brain networks, researchers in the graph
machine learning domain have studied several practical methods
to initialize node features [71, 72]. In this paper, we focus on
the following node features that can be categorized as positional
or structural:

• Identity: A unique one-hot feature vector is initialized
for each node [73, 74]. By giving each ROI in the
brain network a unique high-dimensional vector, this
identity node feature allows the GNN model to learn
the relative positions of the nodes by memorizing their
k-hop neighbors. They are essentially the same as random
initialization considering the parameters in the ﬁrst linear
layer of the GNN are randomly initialized.

• Eigen: Eigen decomposition is performed on the weighted
matrix describing the connection strengths between ROIs
and then the top k eigenvectors are used to generate
a k-dimensional feature vector for each node [75–77].
The optimal value of k is decided by grid search. This
feature is essentially dimension reduction and targets at
grouping brain regions with respect to their positions,
with global graph information condensed into a low-
dimensional representation.

• Degree: The degree value of each node is obtained as a
one-dimensional vector as the node feature. This feature
captures structural information of brain regions, meaning
that neighborhood structural similarity of two regions will
be partially recorded in the initialized node features.
• Degree proﬁle: This method takes advantages of existing
local statistical measures on degree proﬁles [78], where
each feature xi of node vi on graph Gn is computed as

xi = [deg(vi) (cid:107) min(Di) (cid:107) max(Di)
(cid:107) mean(Di) (cid:107) std(Di)],

(1)

where Di = {deg(vi) | (i, j) ∈ En} describes the degree
values of node vi’s one-hop neighborhood and (cid:107) denotes
concatenation.

• Connection proﬁle: The corresponding row for each node
in the edge weight matrix is utilized as the initial node
feature, which contains connections with respect to all
other nodes in the brain network. This feature aligns with
the common practice of using pairwise connections to
perform brain parcellation. Also, it reﬂects the whole
picture of structural or functional information in the brain
network.

B. Message Passing Mechanisms

The power of most GNNs to learn structures lies in their
message passing schemes, where the node representation is
updated iteratively by aggregating neighbor features through
local connections. In each layer l, the node representation hl
i
is updated through two steps, namely message passing and
update respectively. In the message passing step (Eq. 2), each
node vi receives messages from all its neighbors, and then all
the messages are aggregated with a sum function:
(cid:1) ,

(cid:0)hl

ml

(cid:88)

(cid:88)

mij =

Ml

(2)

j, wij

i, hl

i =

j∈Ni

j∈Ni

where Ni denotes the neighbors of node vi in graph G, wij
represents the edge weights between node vi and vj, Ml is the
message function. In the update step (Eq. 3), the embedding of
each node is updated based on the aggregated messages from
Eq. 2 and optionally the previous embedding of node vi, where
the update function can be arbitrary differentiable functions
(e.g., concat the aggregated message with the previous node
embedding and then pass them into another learnable linear
layer).

hl+1

i = Ul

(cid:0)hl

i, ml
i

(cid:1) ,

(3)

where Ul stands for the update function and the number of
running steps L is deﬁned by the number of GNN layers. The
message passing mechanism can leverage both permutation
equivariance and inductive bias towards learning local structures
and achieve good generalization on new graphs. For brain
networks, whether incorporating connections into the message
function is beneﬁcial for graph-level prediction tasks remains
to be investigated. In this paper, we discuss the inﬂuence of
different message function Ml designs including:

• Edge weighted: The message mij passed from node vj
to node vi is calculated as the representation of node vj
weighted by the corresponding edge weight wij, that is

mij = hj · wij.

(4)

This is the standard message passing implementation in
Graph Convolutional Network (GCN) [14] when wij =
1/Ni. With this message vector design, the update of each
brain region representation is inﬂuenced by its neighbor
regions weighted by the connection strength between them.
• Bin concat: In this scheme, we map the edge wij into one
of the equally split T buckets based on its weight value.
Each bucket corresponds to a learnable representation bt,
t = {1 . . . T }. The total bucket number encompassing the
entire value range of edge weights is determined by grid
search and the representation dimension of each bin is
set to the same as node features. Speciﬁcally, given the
number of buckets is T , we ﬁrst rank all the edge weights
and then divide them into the equally divided T buckets
from the lowest to the highest. All those edges falling into
the same bucket will be mapped to the same learnable
edge representation bt, so region connections with similar
strength are binned together. In our experiment, we simply
select from [5, 10, 15, 20] as the possible number of
buckets for grid search, which is a common practice in
machine learning for hyperparameter tuning. The message
mj passed from node vj to node vi is calculated as
the concatenation of the representation of node vj and
its corresponding bucket representation bt followed by a
MLP,

mij = MLP(hj (cid:107) bt).

(5)

The usage of bins helps to clusters region connections
with similar strengths. By concatenating with the unique
neighbor node representation, this message captures both
common and peculiar characteristics of each neighbor.
• Edge weight concat: The message mij passed from node
vj to node vi is represented as the concatenation of the

CUI et al.: BRAINGB: BENCHMARK FOR BRAIN NETWORK ANALYSIS WITH GRAPH NEURAL NETWORKS

7

representation of node vj and the scaled edge weight
d · wij, followed by a MLP,

mij = MLP(hj (cid:107) d · wij),

(6)

where d is a constant equals to the dimension number
of node features. The motivation behind edge weight
scaling is to increase the inﬂuence of edge features to
the same scale as node features. Compared with bin
concat design where edges with weight values falling
into the same bin interval share the same initial edge
representation, directly concatenating the scaled edge
weights as the edge representations can retain the original
edge information, therefore reserving more uniqueness on
the pairwise connection when performing the aggregation
from neighboring brain regions.

• Node edge concat: To investigate the inﬂuence of preserv-
ing the brain region representation from the last time step
while iterative updating the new representation, we design
a message mj as the concatenation of both embeddings
of node vi, vi and the edge weight wij between them,
followed by a MLP, that is

mij = MLP(hi (cid:107) hj (cid:107) wij).

(7)

In this paradigm, every message passed from the local
neighbors of each central node is reinforced with its
representation from the last time step. This design may
alleviate the over-smoothing problem of GNNs, where the
feature distance between all nodes becomes too close and
not distinguishable after layers of convolutions.

• Node concat: Since the effect of involving connection
weights into message passing is still unknown, we also
include another message mij design similar to node edge
concat but without the concatenation of edge weights,
where

mij = MLP(hi (cid:107) hj).

(8)

C. Attention-Enhanced Message Passing

Attention is arguably one of the most important mechanisms
in modern deep learning [79, 80]. It is inspired by human
cognitive systems that tend to selectively concentrate on the
important parts as needed when processing large amounts of
information. Various ﬁelds in deep learning communities such
as natural language processing [81] and computer vision [82]
have widely beneﬁted from attention mechanisms in terms of
model efﬁciency and accuracy. The attention mechanism can
also be used to enhance the message passing scheme of GNNs,
while also providing interpretations over the edge importance.
Speciﬁcally in brain network analysis, by utilizing the
attention-enhanced version of message passing, the model
updates the brain region representation in a data-driven way,
where adjustable attention weights from each local neighbor
perform as an additional inﬂuence factor besides the neural
signals represented by edge weights. It is worth noting that the
traditional designs of graph attention mechanisms on general
graphs usually do not take the edge attributes (i.e., connection
weights in the brain network scenario) into consideration.

However, for brain networks, the correlation between two
regions contains meaningful biomedical information and might
be helpful for graph-level tasks. In this paper, we design several
attention-enhanced message passing mechanisms including:

• Attention weighted: This is the original GAT [16] on
general graphs without involving edge attributes. The
message from node vj to vi is calculated as the node
representation vj weighted by the corresponding attention
weights αij,

mij = hj · αij.

(9)

The αij is calculated from a single-layer feed-forward
neural network parameterized by a weight vector a,
followed by the LeakyReLU nonlinearity σ,

αij =

exp (cid:0)σ (cid:0)a(cid:62) [Θxi (cid:107) Θxj](cid:1)(cid:1)
k∈N (i)∪{i} exp (σ (a(cid:62) [Θxi (cid:107) Θxk]))

(cid:80)

,

(10)

where Θ represents a learnable linear transformation
matrix.

• Edge weighted w/ attn: This is the attention-enhanced
version of edge weighted message passing in Eq. 4. The
message from vj to vi is obtained as the multiplication
of node vj’s representation hj, the edge weight wij and
the attention score αij in Eq. 10,

mij = hj · αij · wij.

(11)

• Attention edge sum: This is another version of attention-
enhanced edge weighted (Eq. 4) message passing. The
edge weight wij and the attention score αij are ﬁrst
summed, then used as the impact factor on the node
embedding hj,

mij = hj · (αij + wij).

(12)

• Node edge concat w/ attn: This is the attention-enhanced
version of node edge concat (Eq. 7) message passing,
where the attention score αij (Eq. 10) between node vi
and vj is multiplied on the node representation hj before
concatenation, followed by a MLP,

mij = MLP(hi (cid:107) (hj · αij) (cid:107) wij).

(13)

• Node concat w/ attn: This design corresponds to the
attention-enhanced version of node concat (Eq. 8) message
passing, where the attention score αij (Eq. 10) between
node vi and node vj is multiplied on the node representa-
tion hj before concatenation, followed by a MLP,

mij = MLP(hi (cid:107) (hj · αij)).

(14)

D. Pooling Strategies

In the second phase of GNNs, a feature vector for the whole

graph gn is computed using the pooling strategy R, where

gn = R ({hk | vk ∈ Gn}) .

(15)

The pooling function R operates on the set of node vectors
and is invariant to permutations of the node vectors. In this
paper, we cover three basic global pooling operators [83, 84]:

8

IEEE TRANSACTIONS ON MEDICAL IMAGING (UNDER REVISION)

• Mean pooling: The graph-level representation is obtained
by averaging node features. For each single graph Gn, the
graph-level representation is computed as

gn =

1
M

M
(cid:88)

k=1

hk.

(16)

• Sum pooling: The graph-level representation is obtained
by summing up all node features. For each single graph
Gn, the graph-level representation is computed as

gn =

M
(cid:88)

k=1

hk.

(17)

• Concat pooling: The graph-level representation is obtained
by concatenating node features of all nodes contained
in the graph. For each single graph Gn, the graph-level
representation is computed as

gn = (cid:107)M

k=1 hk = h1 (cid:107) h2 (cid:107) . . . (cid:107) hM .

(18)

Note that there are also other complex pooling strategies such as
hierarchical pooling [85], learnable pooling [86] and clustering
readout [87], which are usually viewed as independent GNN
architecture designs that are not deﬁned based on combinative
modules. Here we include the representative method of
DiffPool [85] to provide a view of the comparison between
basic and more complex pooling methods.

V. EXPERIMENTAL ANALYSIS AND INSIGHTS

In this section, we show experimental results on brain
networks generated from real-world neuroimaging studies
with different GNN modular designs. Varying each design
dimension under each module results in a total of 375 different
architectures. Note that here we do not target at covering
all the combinations, but with an ambition to quickly ﬁnd a
good design choice for a speciﬁc dataset or a downstream
task. Furthermore, we emphasize that the design space can be
expanded as new design dimensions emerge in state-of-the-art
models.

A. Experimental Settings

1) Datasets: To establish a benchmark for generic brain
network analysis models, we include four datasets processed
and constructed from different neuroimaging modalities, specif-
ically fMRI (HIV [38], PNC20, ABCD21) and dMRI (PPMI22),
based on different brain atlas. For the HIV and PPMI datasets,
the task is to classify patients from healthy control (Patient,
Normal Control); while for the PNC and ABCD datasets, the
task is gender prediction (Male, Female). We intentionally
cover such a diverse set of datasets from different modalities
(and preprocessing procedures/parcellations/tasks), because our
purpose is to establish a benchmark for generic brain network
analysis models. Thus observations on a diverse set of datasets
can be more instructive for methodology focused studies. All
the datasets we used have been visually checked by imaging

20https://www.nitrc.org/projects/pnc
21https://nda.nih.gov/abcd
22https://www.ppmi-info.org

experts in our team for quality control. Among these four
datasets, PNC, PPMI, and ABCD are restrictively publicly
available ones that can be requested and downloaded from
their ofﬁcial website. The dataset information is summarized
in TABLE I. Since the datasets can be acquired from multiple
sites, multisite issues need to be addressed when performing the
analysis on the constructed networks. Over the past few years,
ComBat techniques [88, 89] from the microarray literature have
started to be used more frequently to deal with multi-site batch
effects. Since our benchmark focuses more on a comprehensive
overview of brain network construction and effective GNN
designs for brain networks, advanced methods for handling
multi-site issues are out of the scope of this work. Interested
readers can refer to [90–94] for more advanced multisite data
handling methods.

• Human Immunodeﬁciency Virus Infection (HIV): This
dataset is collected from the Chicago Early HIV Infection
Study at Northwestern University. The clinical cohort in-
cludes fMRI imaging of 70 subjects, 35 of which are early
HIV patients and the other 35 are seronegative controls.
The preprocessing includes realignment to the ﬁrst volume,
followed by slice timing correction, normalization, and
spatial smoothness, band-pass ﬁltering, and linear trend
removal of the time series. We focus on the 116 anatomical
ROIs [95] and extract a sequence of time courses from
them. Finally, brain networks with 90 cerebral regions
are constructed, with links representing the correlations
between ROIs.

• Philadelphia Neuroimaging Cohort (PNC): This rs-
fMRI dataset is from the Brain Behavior Laboratory
at the University of Pennsylvania and the Children’s
Hospital of Philadelphia. 289 (57.46%) of the 503 included
subjects are female, indicating this dataset is balanced
across genders. The regions are parcellated based on
the 264-node atlas deﬁned by Power et al. [96]. The
preprocessing includes slice timing correction, motion
correction, registration, normalization, removal of linear
trends, bandpass ﬁltering, and spatial smoothing. In the
resulting data, each sample contains 264 nodes with time-
series data collected through 120 time steps. We focus on
the 232 nodes in the Power’s atlas associated with major
resting-state functional modules [97].

• Parkinson’s Progression Markers Initiative (PPMI): This
dataset is from a collaborative study for Parkinson’s
Research to improve PD therapeutics. We consider the
DTI acquisition of 754 subjects, with 596 Parkinson’s
disease patients and 158 healthy controls. The raw data
are ﬁrst aligned to correct for head motion and eddy
current distortions. Then the non-brain tissue is removed
and the skull-stripped images are linearly aligned and
registered. 84 ROIs are parcellated from T1-weighted
structural MRI based on the Desikan-Killiany’ cortical
atlas [98] and the brain network is reconstructed using the
deterministic 2nd-order Runge-Kutta (RK2) whole-brain
tractography algorithm [64].

• Adolescent Brain Cognitive Development Study (ABCD):
This study recruits children aged 9-10 years across 21

CUI et al.: BRAINGB: BENCHMARK FOR BRAIN NETWORK ANALYSIS WITH GRAPH NEURAL NETWORKS

9

TABLE I: Dataset summarization.

Dataset Modality

# Samples

Atlas

Size

Response

# Classes

HIV
PNC
PPMI
ABCD

fMRI
fMRI
DTI
fMRI

70
503
754
7,901

AAL 116
Power 264
Desikan-Killiany
HCP 360

90 × 90
232 × 232
84 × 84
360 × 360

Disease
Gender
Disease
Gender

2
2
2
2

TABLE II: Performance report (%) of different message passing GNNs in the four-modular design space with other two
representative baselines on four datasets. We highlight the best performed one in each module based on AUC, since it is not
sensitive to the changes in the class distribution, providing a fair evaluation on unbalanced datasets like PPMI.

Module

Method

Node
Features

Message
Passing

Identity
Eigen
Degree
Degree proﬁle
Connection proﬁle

Edge weighted
Bin concat
Edge weight concat
Node edge concat
Node concat

Message
Passing
w/ Attention

Attention weighted
Edge weighted w/ attn
Attention edge sum
Node edge concat w/ attn
Node concat w/ attn

Pooling
Strategies

Shallow
Baselines

Deep
Baselines

Mean pooling
Sum pooling
Concat pooling
DiffPool

M2E
MPCA
MK-SVM

BrainNetCNN
BrainGNN

Accuracy

50.00±0.00
65.71±2.86
44.29±5.35
50.00±0.00
65.71±13.85

50.00±0.00
50.00±0.00
51.43±2.86
65.71±13.85
70.00±15.91

50.00±0.00
50.00±0.00
51.43±7.00
72.86±11.43
71.43±9.04

47.14±15.39
57.14±9.04
65.71±13.85
72.86±21.19

57.14±19.17
67.14±20.25
65.71±7.00

60.21±17.16
62.98±11.15

HIV

F1

33.33±0.00
65.45±2.69
35.50±6.10
33.33±0.00
64.11±13.99

33.33±0.00
33.33±0.00
44.36±6.88
64.11±13.99
68.83±17.57

33.33±0.00
33.33±0.00
49.13±5.65
72.52±11.72
70.47±9.26

41.71±17.36
52.23±12.65
64.11±13.99
70.22±23.91

53.71±19.80
64.28±23.47
62.08±7.49

60.12±13.56
60.45±8.96

AUC

Accuracy

46.73±10.57
65.31±2.89
42.04±4.00
50.00±0.00
75.10±16.95

49.80±4.20
49.39±9.25
48.16±10.13
75.10±16.95
77.96±8.20

49.80±8.52
42.04±15.63
54.49±15.67
78.37±10.85
82.04±11.21

58.78±18.63
57.96±11.15
75.10±16.95
76.57±17.16

57.50±18.71
69.17±20.17
65.83±7.41

70.93±4.01
68.03±9.16

57.34±0.17
51.40±3.92
63.89±2.27
51.40±7.21
69.83±4.15

64.87±5.44
54.74±5.88
63.68±3.31
69.83±4.15
70.63±2.35

65.09±2.21
62.90±1.22
61.51±2.86
67.66±5.07
68.85±6.42

66.86±2.33
60.13±2.87
69.83±4.15
62.72±12.40

53.76±4.94
76.76±4.30
78.38±5.09

71.93±4.90
70.62±4.85

PNC

F1

36.44±0.17
48.63±5.42
59.69±3.85
33.80±3.21
66.20±4.74

59.70±7.04
36.42±3.97
60.27±5.97
66.20±4.74
67.12±1.81

60.74±4.89
61.14±0.57
55.36±4.76
64.69±5.36
64.29±10.15

61.39±4.88
53.96±7.61
66.20±4.74
75.95±4.28

46.10±6.94
75.95±4.28
77.55±5.83

69.94±5.42
68.93±4.01

PPMI

ABCD

AUC

Accuracy

F1

AUC

Accuracy

F1

AUC

52.58±4.80
50.18±7.57
70.25±4.38
50.00±0.00
76.69±5.04

69.98±4.19
61.68±3.91
67.34±3.02
76.69±5.04
78.32±1.42

69.79±4.24
69.74±2.37
69.38±3.50
74.52±1.20
75.36±5.09

74.20±3.39
66.11±4.22
76.69±5.04
64.08±16.71

49.70±5.18
76.05±4.34
77.57±5.65

78.50±3.28
77.53±3.23

79.25±0.24
74.09±2.77
79.52±2.31
77.02±1.97
77.99±2.78

79.25±0.24
79.25±0.24
79.25±0.24
77.99±2.78
78.41±1.62

79.25±0.24
79.25±0.24
79.11±0.40
77.30±1.52
78.41±1.43

79.25±0.24
79.39±0.52
77.99±2.78
78.42±3.53

78.69±1.78
79.15±0.57
79.15±0.57

77.24±2.09
79.17±1.22

44.21±0.08
47.36±4.26
49.40±5.17
49.45±3.51
52.96±4.52

44.21±0.08
44.21±0.08
44.21±0.08
52.96±4.52
54.46±3.08

44.21±0.08
44.21±0.08
44.17±0.12
50.96±4.20
49.98±1.87

44.21±0.08
47.68±3.12
52.96±4.52
56.55±6.48

45.81±4.17
44.18±0.18
44.18±0.18

50.24±3.09
44.19±3.11

59.65±6.80
49.21±1.58
59.73±4.31
58.65±2.44
65.77±4.09

62.26±2.80
52.67±7.16
59.72±4.65
65.77±4.09
68.34±1.89

63.24±3.77
54.92±4.80
60.47±6.26
63.93±4.89
68.14±5.01

59.64±5.47
61.29±2.11
65.77±4.09
63.07±7.77

50.39±2.59
50.00±0.00
50.00±0.00

58.76±8.95
45.26±3.65

49.97±0.13
50.79±0.82
63.46±1.29
49.92±0.11
82.42±1.93

74.47±1.17
53.72±4.97
64.59±1.30
82.42±1.93
80.50±2.27

77.74±0.97
78.04±1.96
75.71±1.52
83.10±0.47
83.19±0.93

81.13±0.35
77.48±3.75
82.42±1.93
76.45±1.44

50.10±1.90
88.94±1.64
89.42±0.97

85.1±0.92
OOM

33.32±0.06
50.79±0.83
63.45±1.28
33.30±0.05
82.30±2.08

74.36±1.23
43.26±12.43
64.30±1.43
82.30±2.08
80.10±2.47

77.70±1.01
77.81±2.33
75.59±1.68
83.03±0.52
83.12±0.96

81.06±0.34
76.96±4.58
82.30±2.08
76.35±1.52

49.95±1.88
88.94±1.64
89.42±0.97

85.7±0.83
OOM

50.00±0.20
51.18±1.16
68.16±1.41
50.00±0.00
91.33±0.77

82.37±1.46
61.86±5.79
70.63±1.02
91.33±0.77
91.36±0.92

85.10±1.10
86.86±0.63
83.78±0.82
91.85±0.29
91.55±0.59

88.49±1.12
87.90±0.65
91.33±0.77
83.92±1.25

50.10±1.90
88.94±1.64
89.42±0.97

93.5±0.34
OOM

sites in the U.S. Each child is followed into early adult-
hood, with repeated imaging scans, as well as extensive
psychological and cognitive testing [99]. After selection,
7,901 children are included in the analysis, and 3,961
(50.1%) among them are female. We use rs-fMRI scans
for the baseline visit processed with the standard and
open-source ABCD-HCP BIDS fMRI Pipeline23. After
processing, each sample contains a connectivity matrix
whose size is 360 × 360 and BOLD time-series for each
node. The region deﬁnition is based on the HCP 360 ROI
atlas [100].

Structural connectivity and functional connectivity are dif-
ferent in their strength and sparsity, thus need to be handled
differently. For structural connectivity, we normalize the edge
weights by dividing each value by the maximum value in a
sample. The processed edge weights are thus ranged from 0
to 1. For functional connectivity, we follow common practice
to remove the negative values for GNNs that cannot handle
negative values (like GCN), and keep them for GNNs that can
handle negative values (like GAT).

2) Baselines: For comprehensiveness, we compare our
modular design with competitors of both shallow and deep
models. The shallow methods we consider include M2E [7],
MPCA [101], and MK-SVM [102], where the output graph-
level embeddings are evaluated using logistic regression
classiﬁers. Speciﬁcally, M2E is a partially-symmetric tensor

23https://github.com/DCAN-Labs/abcd-hcp-pipeline

factorization based method for brain network analysis, and
it has been empirically compared with spectral embedding
clustering methods such as SEC [103] or spectral learning
frameworks such as AMGL [104]; MPCA is proposed for
the feature extraction and analysis of tensor objects such as
neuroimaging; multiple kernel SVM (MK-SVM) is essentially
an extension of the conventional SVM algorithm and has
been applied for the analysis of functional and structural
connectivity in Alzheimer’s disease. We also include two state-
of-the-art deep models speciﬁcally designed for brain networks:
BrainGNN [1] and BrainNetCNN [17]. The message passing in
BrainGNN is Edge weighted and it further leverages additional
regional information (such as coordinates or ROI ordering
based one-hot embeddings) to assign a separate GCN kernel
for each ROI where ROIs in the same community are embedded
by the similar kernel and those in different communities are
embedded in different ways, but this will introduce a lot of
additional model parameters and make the model hard to train.
On the other hand, BrainNetCNN models the adjacency matrix
of a brain network as a 2D image and does not follow the
message passing mechanism as we discussed in Section IV-B.
Note that the purpose of our paper, and of most benchmark
papers, is not to establish superior performance of a certain
method, but rather to provide an effective and fair ground for
comparing different methods.

3) Implementation Details: The proposed model is imple-
mented using PyTorch 1.10.2 [105] and PyTorch Geometric

10

IEEE TRANSACTIONS ON MEDICAL IMAGING (UNDER REVISION)

2.0.3 [106]. A Quadro RTX 8000 GPU with 48GB of memory
is used for model training. The optimizer we used is Adam.
We train all of our models through 20 epochs, and the learning
rate is 1e-3. We use a weight decay of 1e-4 as a means
of regularization. The loss function is cross entropy. Hyper-
parameters are selected automatically with an open-source
AutoML toolkit NNI24. Please refer to our repository for
comprehensive parameter conﬁgurations. When tuning the
hyperparameters, we ﬁrst split the dataset into a train set
and a test set with the ratio of 8:2. The k-fold validation
is performed on the train set, where we further divide the
train set into 10 parts and take one in each run to use as the
validation set. The selection of the best hyperparameter is based
on the average performance of the model on the validation
sets. The reported metrics in Table II, on the other hand, is the
average performance on the test set, with each run trained on
different train sets. The competing methods are also tuned in
the same way. For BrainGNN, we used the author’s open-source
code25. For BrainNetCNN, we implemented it by ourselves
with PyTorch, which is publicly available in our BrainGB
package26. For the hyper-parameter tuning, we selected several
important hyper-parameters and performed the grid search on
them based on the provided best setting as claimed in their
paper. To be speciﬁc, for BrainGNN, we searched for different
learning rates in {0.01, 0.005, 0.001} with different feature
dimensions in {100, 200} and the number of GNN layers in
{2, 3}. For BrainNetCNN, we searched for different dropout
rates in {0.3, 0.5, 0.7} with learning rates in {0.001, 0.0005,
0.0001} and the number of layers in MLP in {1, 2, 3}. The
reported results of these two baselines in Table II are from
the best performing groups, where for BrainGNN, the learning
rate is 0.01, the feature dimension is 200 and the number
of GNN layers is 2, and for BrainNetCNN, the dropout rate
is 0.3, the learning rate is 0.0001 and the number of layers
in MLP is 3. The metrics used to evaluate performance are
Accuracy, F1 score, and Area Under the ROC Curve (AUC),
which are widely used for disease identiﬁcation. To indicate
the robustness of each model, all the reported results are the
average performance of ten-fold cross-validation conducted on
different train/test splits.

B. Performance Report

1) Node Feature: On comparing node features, we set the
other modules as the well-performed settings in individual
tests. Speciﬁcally, we use node edge concat in Eq. 7 as the
message passing scheme, and concat pooling in Eq. 18 as
the pooling strategy. Our experimental results demonstrate
that the connection proﬁle which uses the corresponding
row in the adjacency matrix as the node features achieves
the best performance across all datasets, with up to 33.99%
improvements over the second-best, degree, on ABCD. We
believe this is because the connection proﬁle captures the
whole picture of structural information in the brain network,
and preserves rich information on pairwise connections that can

24https://github.com/microsoft/nni
25https://github.com/xxlya/BrainGNN_Pytorch
26https://github.com/HennyJie/BrainGB

be used to perform brain parcellation. In general, the structure
node features (e.g., degree, connection proﬁle) perform better
than the positional ones (e.g., identity, eigen), indicating that
the overall structural information of graph and the structural
role of each node are important in the task of brain network
analysis. This conclusion is consistent with previous ﬁndings
in the literature that structural artiﬁcial node features work
well for graph-level tasks on general graphs [71].

2) Message Passing: To study the effectiveness of different
message passing schemes, we initialize the node features with
connection proﬁle and apply the concat pooling to produce
graph-level representations, which both perform best when
examined separately in each module. Our results reveal that
node concat (Eq. 8) message passing has the highest AUC
performance across four datasets, followed by node edge concat
(Eq. 7), which achieves a similar AUC performance with
sometimes slightly better accuracy and F1 scores (ABCD).
The performance superiority of the last two methods may arise
from their advantage of reinforcing self-representation of the
central node during each step of message passing. This helps to
retain the original information from the last step and avoid over-
ﬁtting towards a biased direction in the optimization process.
Surprisingly, the edge involved node edge concat performs
slightly worse than the pure node concat, though the gap gets
closer on larger datasets. This indicates that encoding edge
weights as a single value may not be useful when the global
structure has already been used as the initial node features.

3) Attention Enhanced Message Passing: When evaluating
the effectiveness of different attention-enhanced message
passing schemes, we set the node features as connection proﬁle
and apply the concat pooling strategy, just as for the evaluation
of message passing without attention mechanisms. It is shown
that the node concat w/ attn (Eq. 14) and node edge concat w/
attn (Eq. 13) yield very close results across four datasets and
they alternately perform the best. Furthermore, the attention-
enhanced version achieves better outcomes most of the time (up
to 5.23% relative improvements) vs. the corresponding message
passing architecture without an attention mechanism. This
demonstrates the effectiveness of utilizing learnable attention
weights in the GNN aggregation and update process in addition
to the ﬁxed edge weights. Also, the node edge concat w/
attn surpasses node concat w/ attn on the larger dataset (e.g.,
ABCD), which may imply potential advantages of involving
edge weights into message design when there are enough
training samples.

4) Pooling Strategies: For studying pooling strategies, we
employ the node edge concat (Eq. 7) as the message passing
scheme and connection proﬁle as the initial node features.
Our ﬁndings reveal that the concat pooling strategy (Eq. 18)
consistently outperforms the other two methods across all
four datasets. This is likely because when concat is used, the
ﬁnal node representations of all the brain regions are kept in
the graph-level representation for classiﬁers. The other two
paradigms, on the other hand, obtain a graph-level embedding
with the same dimension of node features. Thus they lose some
information that could be helpful for graph-level prediction
tasks. Though concat does not ensure permutation invariance, it
is actually not needed for brain network analysis since the node

CUI et al.: BRAINGB: BENCHMARK FOR BRAIN NETWORK ANALYSIS WITH GRAPH NEURAL NETWORKS

11

order given a parcellation is ﬁxed. The compared hierarchical
pooling method DiffPool demonstrates some advantages on
the small HIV dataset but fails to surpass the simple concat
pooling on three other larger datasets.

5) Other Baselines: In general, we expect deep models like
GNNs to perform better on larger datasets. For example, the
performance of GNN models on the ABCD dataset clearly
surpasses all shallow models by about 2 percent. However,
this trend should not prohibit one from experimenting with
GNN models on smaller datasets. GNNs do perform well
on some small datasets, such as the HIV dataset. Despite
running on a small dataset, GNN models in BrainGB have an
over 5 percent advantage over all shallow models. As for the
deep baselines, BrainGNN can be out-of-memory (OOM) on
large datasets. The best combination based on our modular
design outperforms BrainGNN on all four datasets (HIV,
PNC, PPMI and ABCD) and achieves comparable results with
BrainNetCNN in most cases especially on smaller datasets (HIV,
PPMI). These ﬁndings prove the need to carefully experiment
with our modular designs of GNNs before further developing
more complicated architectures, which might just overﬁt certain
datasets.

6) Insights on Density Levels: Functional connectivity and
structural connectivity have distinctive differences in sparsity
levels. Functional networks like ABCD are fully connected.
Structural networks like PPMI contain approximately 22.64%
edges on average. Through our experiments, we found sparsity
levels do have an impact on the choices of hyperparameters.
For example, GNNs on the sparser structural networks of PPMI
reach the maximum performance with a hidden dimension of
64, whereas on the functional network of ABCD, they have an
optimal hidden dimension of 256, which indicates that GNN
models should more complicated with more learnable param-
eters when the input networks are denser. This observation
can be instructive for designing GNN architectures on brain
networks constructed from different modalities.

VI. OPEN SOURCE BENCHMARK PLATFORM

To foster future research, we provide an out-of-box package
that can be directly installed through pip, with installation and
tutorials on our hosted BrainGB website https://brai
ngb.us. The BrainGB package is also open-sourced at ht
tps://github.com/HennyJie/BrainGB. We provide
examples of GNN-based brain network analysis, trained models,
and instructions on imaging preprocessing and functional and
structural brain networks construction from raw fMRI and
dMRI respectively. It is noted that due to the modular designs,
BrainGB can also be extended to other tasks, by adding task-
speciﬁc functions in each module.

VII. DISCUSSION AND EXTENSIONS

In this paper, we ﬁrst present BrainGB, a uniﬁed, modular,
scalable, and reproducible framework for brain network analy-
sis with GNNs. While the dataset generation, baselines, and
evaluations we provide in BrainGB are thorough, we consider
several limitations in the current paradigm:

• The aggregation mechanism in GNN is known to be
effective for node-level tasks with the effect of node
feature smoothing, and for graph-level
tasks due to
its capability in structure differentiation. However, for
brain networks, what kinds of graph structures (e.g.,
communities, subgraphs) are effective beyond the pairwise
connections are still unknown.

• The small size of neuroimaging datasets may limit the
effectiveness and generalization ability of complex deep
learning models.

Towards these two limitations, we envision several future
directions that can be potentially helpful to fully unleash the
power of GNNs for brain network analysis:
• Neurology-driven GNN designs:

to design the GNN
architectures based on neurological understandings of
predictive brain signals, especially disease-speciﬁc ones.
• Pre-training and transfer learning of GNNs: to design tech-
niques that can train complex GNN models across studies
and cohorts [107]. Besides, information sharing across
different diseases could lead to a better understanding of
cross-disorder commonalities.

ACKNOWLEDGMENT

This research was supported in part by the Univer-
sity Research Committee of Emory University, and the
internal funding and GPU servers provided by the Com-
puter Science Department of Emory University. The au-
thors gratefully acknowledge support from National Institutes
of Health (R01MH105561, R01MH118771, R01AG071243,
R01MH125928, U01AG068057), National Science Foundation
(IIS 2045848, IIS 1837956) and Ofﬁce of Naval Research
(N00014-18-1-2009). The content is solely the responsibility
of the authors and does not necessarily represent the ofﬁcial
views of the NIH, NSF, and ONR.

Support for the collection of the Philadelphia Neurode-
velopmental Cohort (PNC) dataset was provided by grant
RC2MH089983 awarded to Raquel Gur and RC2MH089924
awarded to Hakon Hakorson. All subjects were recruited
through the Center for Applied Genomics at The Children’s
Hospital in Philadelphia. The ABCD Dataset used in the
preparation of this article were obtained from the Adolescent
Brain Cognitive Development (ABCD) Study (https://ab
cdstudy.org), held in the NIMH Data Archive (NDA).
This is a multisite, longitudinal study designed to recruit more
than 10,000 children age 9-10 and follow them over 10 years
into early adulthood. The ABCD Study® is supported by the
National Institutes of Health and additional federal partners
under award numbers U01DA041048, U01DA050989, U01
DA051016, U01DA041022, U01DA051018, U01DA05103
7, U01DA050987, U01DA041174, U01DA041106, U01DA
041117, U01DA041028, U01DA041134, U01DA050988, U
01DA051039, U01DA041156, U01DA041025, U01DA041
120, U01DA051038, U01DA041148, U01DA041093, U01
DA041089, U24DA041123, U24DA041147. A full list of
supporters is available at https://abcdstudy.org/fe
deral-partners.html. A listing of participating sites
and a complete listing of the study investigators can be found at

12

IEEE TRANSACTIONS ON MEDICAL IMAGING (UNDER REVISION)

https://abcdstudy.org/consortium_members/.
ABCD consortium investigators designed and implemented the
study and/or provided data but did not necessarily participate in
the analysis or writing of this report. This manuscript reﬂects
the views of the authors and may not reﬂect the opinions or
views of the NIH or ABCD consortium investigators. The
ABCD data repository grows and changes over time. The
ABCD data used in this report came from NIMH Data Archive
Release 4.0 (DOI 10.15154/1523041). DOIs can be found at
https://nda.nih.gov/abcd.

REFERENCES

[1] X. Li, Y. Zhou, N. Dvornek, M. Zhang, S. Gao, J. Zhuang, D. Scheinost,
L. H. Staib, P. Ventola, and J. S. Duncan, “Braingnn: Interpretable brain
graph neural network for fmri analysis,” Med Image Anal, 2021. 1, 3, 9
[2] F. V. Farahani, W. Karwowski, and N. R. Lighthall, “Application of graph
theory for identifying connectivity patterns in human brain networks: a
systematic review,” Front. Neurosci., vol. 13, p. 585, 2019. 1

[3] K. Osipowicz, M. R. Sperling, A. D. Sharan, and J. I. Tracy, “Functional
mri, resting state fmri, and dti for predicting verbal ﬂuency outcome
following resective surgery for temporal lobe epilepsy,” J. Neurosurg.,
vol. 124, pp. 929–937, 2016. 1

[4] L. A. Maglanoc, T. Kaufmann, R. Jonassen, E. Hilland, D. Beck,
N. I. Landrø, and L. T. Westlye, “Multimodal fusion of structural
and functional brain imaging in depression using linked independent
component analysis,” Hum Brain Mapp, vol. 41, pp. 241–255, 2020. 1,
5

[5] E. Bullmore and O. Sporns, “Complex brain networks: graph theoretical
analysis of structural and functional systems,” Nat. Rev. Neurosci.,
vol. 10, pp. 186–198, 2009. 1, 3

et al., “Linked sex differences in cognition and functional connectivity
in youth,” Cereb. Cortex, vol. 25, pp. 2383–2394, 2015.

[21] G. Deco, V. K. Jirsa, and A. R. McIntosh, “Emerging concepts for the
dynamical organization of resting-state activity in the brain,” Nat. Rev.
Neurosci., vol. 12, pp. 43–56, 2011.

[22] Y. Wang and Y. Guo, “A hierarchical independent component analysis
model for longitudinal neuroimaging studies,” NeuroImage, vol. 189,
pp. 380–400, 2019.

[23] R. Yu, L. Qiao, M. Chen, S.-W. Lee, X. Fei, and D. Shen, “Weighted
graph regularized sparse brain network construction for mci identiﬁca-
tion,” Pattern Recognit, vol. 90, pp. 220–231, 2019. 2

[24] T. R. Insel and B. N. Cuthbert, “Brain disorders? precisely,” Science,

vol. 348, pp. 499–500, 2015. 2

[25] L. M. Williams, “Precision psychiatry: a neural circuit taxonomy for
depression and anxiety,” Lancet Psychiatry, vol. 3, pp. 472–480, 2016.
[26] W. Li, M. Wang, Y. Li, Y. Huang, and X. Chen, “A novel brain network
construction method for exploring age-related functional reorganization,”
Comput. Intell. Neurosci., vol. 2016, 2016. 2

[27] J. Zimmermann, J. D. Grifﬁths, and A. R. McIntosh, “Unique mapping
of structural and functional connectivity on cognition,” J. Neurosci.,
vol. 38, pp. 9658–9667, 2018. 3

[28] Y. Hu, M. Zeydabadinezhad, L. Li, and Y. Guo, “A multimodal
multilevel neuroimaging model for investigating brain connectome
development,” J Am Stat Assoc, pp. 1–15, 2022. 3

[29] G. Martensson, J. B. Pereira, P. Mecocci, B. Vellas, M. Tsolaki,
I. Kłoszewska, H. Soininen, S. Lovestone, A. Simmons, G. Volpe et al.,
“Stability of graph theoretical measures in structural brain networks in
alzheimer’s disease,” Sci. Rep., vol. 8, pp. 1–15, 2018. 3

[30] N. Yahata, J. Morimoto, R. Hashimoto, G. Lisi, K. Shibata,
Y. Kawakubo, H. Kuwabara, M. Kuroda, T. Yamada, F. Megumi et al.,
“A small number of abnormal brain connections predicts adult autism
spectrum disorder,” Nat. Commun., vol. 7, pp. 1–12, 2016.

[31] M. A. Lindquist, “The statistical analysis of fmri data,” Stat Sci, vol. 23,

pp. 439–464, 2008. 3

[32] S. M. Smith, “The future of fmri connectivity,” NeuroImage, vol. 62,

[6] O. Sporns, “Graph theory methods: applications in brain networks,”

pp. 1257–1266, 2012. 3

Dialogues Clin. Neurosci., 2022. 1

[7] Y. Liu et al., “Multi-view multi-graph embedding for brain network

clustering analysis,” in AAAI, 2018. 1, 9

[8] L. Zhan, Y. Liu, Y. Wang, J. Zhou, N. Jahanshad, J. Ye, P. M. Thompson,
and A. D. N. I. (ADNI), “Boosting brain connectome classiﬁcation
accuracy in alzheimer’s disease using higher-order singular value
decomposition,” Frontiers in neuroscience, vol. 9, p. 257, 2015. 1
[9] J. Faskowitz, R. F. Betzel, and O. Sporns, “Edges in brain networks:
Contributions to models of structure and function,” arXiv.org, 2021. 1,
3

[10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:
Transformers for image recognition at scale,” in ICLR, 2021. 1
[11] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
G. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable
visual models from natural language supervision,” in ICML, 2021. 1

[12] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Luˇci´c, and C. Schmid,

“Vivit: A video vision transformer,” in ICCV, 2021. 1

[13] A. Gulati, J. Qin, C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang,
Z. Zhang, Y. Wu, and R. Pang, “Conformer: Convolution-augmented
transformer for speech recognition,” in INTERSPEECH, 2020. 1
[14] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph

convolutional networks,” in ICLR, 2017. 1, 3, 6

[33] R. Shi and Y. Guo, “Investigating differences in brain functional
networks using hierarchical covariate-adjusted independent component
analysis,” Ann Appl Stat, p. 1930, 2016.

[34] T. Dai, Y. Guo, A. D. N. Initiative et al., “Predicting individual
brain functional connectivity using a bayesian hierarchical model,”
NeuroImage, vol. 147, pp. 772–787, 2017.

[35] I. A. Higgins, S. Kundu, K. S. Choi, H. S. Mayberg, and Y. Guo, “A
difference degree test for comparing brain networks,” Hum Brain Mapp,
pp. 4518–4536, 2019. 3

[36] B. Jie, M. Liu, X. Jiang, and D. Zhang, “Sub-network based kernels

for brain network classiﬁcation,” in ICBC, 2016. 3

[37] L. He, K. Chen, W. Xu, J. Zhou, and F. Wang, “Boosted sparse and

low-rank tensor regression,” in NeurIPS, 2018. 3

[38] Y. Liu, L. He, B. Cao, P. Yu, A. Ragin, and A. Leow, “Multi-view
multi-graph embedding for brain network clustering analysis,” in AAAI,
2018. 3, 8

[39] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov,
and M. Welling, “Modeling relational data with graph convolutional
networks,” in ESWC, 2018. 3

[40] S. Wu, Y. Tang, Y. Zhu, L. Wang, X. Xie, and T. Tan, “Session-based
recommendation with graph neural networks,” in AAAI, 2019. 3
[41] X. Li, N. C. Dvornek, Y. Zhou, J. Zhuang, P. Ventola, and J. S. Duncan,
“Graph neural network for interpreting task-fmri biomarkers,” in MICCAI,
2019. 3

[15] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph

[42] A. Bessadok, M. A. Mahjoub, and I. Rekik, “Graph neural networks in

neural networks?” in ICLR, 2019. 3

network neuroscience,” arXiv.org, 2021.

[16] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and

Y. Bengio, “Graph attention networks,” in ICLR, 2018. 1, 7

[17] J. Kawahara, C. J. Brown, S. P. Miller, B. G. Booth, V. Chau, R. E.
Grunau, J. G. Zwicker, and G. Hamarneh, “Brainnetcnn: Convolutional
neural networks for brain networks; towards predicting neurodevelop-
ment,” NeuroImage, vol. 146, pp. 1038–1049, 2017. 1, 3, 9

[18] G. K. Murugesan, C. Ganesh, S. Nalawade, E. M. Davenport, B. Wagner,
W. H. Kim, and J. A. Maldjian, “Brainnet: Inference of brain network
topology using machine learning,” Brain Connect, vol. 10, pp. 422–435,
2020. 2

[19] C. Su, Z. Xu, J. Pathak, and F. Wang, “Deep learning in mental health
outcome research: a scoping review,” Transl. Psychiatry, vol. 10, pp.
1–26, 2020. 2

[43] H. Cui, W. Dai, Y. Zhu, X. Li, L. He, and C. Yang, “Interpretable
graph neural networks for connectome-based brain disorder analysis,”
in MICCAI, 2022.

[44] Y. Zhu, H. Cui, L. He, L. Sun, and C. Yang, “Joint embedding of
structural and functional brain networks with graph neural networks for
mental illness diagnosis,” 2022.

[45] X. Kan, H. Cui, L. Joshua, Y. Guo, and C. Yang, “Fbnetgen: Task-aware
gnn-based fmri analysis via functional brain network generation,” in
MIDL, 2022.

[46] H. Tang, L. Guo, X. Fu, B. Qu, P. M. Thompson, H. Huang, and L. Zhan,
“Hierarchical brain embedding using explainable graph learning,” in
2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI).
IEEE, 2022, pp. 1–5.

[20] T. D. Satterthwaite, D. H. Wolf, D. R. Roalf, K. Ruparel, G. Erus,
S. Vandekar, E. D. Gennatas, M. A. Elliott, A. Smith, H. Hakonarson

[47] H. Tang, L. Guo, X. Fu, B. Qu, O. Ajilore, Y. Wang, P. M. Thompson,
H. Huang, A. D. Leow, and L. Zhan, “A hierarchical graph learning

CUI et al.: BRAINGB: BENCHMARK FOR BRAIN NETWORK ANALYSIS WITH GRAPH NEURAL NETWORKS

13

model for brain network regression analysis,” Frontiers in Neuroscience,
vol. 16, 2022. 3

[48] S. Sarraf and J. Sun, “Functional brain imaging: A comprehensive

survey,” arXiv.org, 2016. 3

[49] A. Bernstein, R. Akzhigitov, E. Kondrateva, S. Sushchinskaya, I. Samo-
taeva, and V. Gaskin, “MRI brain imagery processing software in data
analysis,” Trans. Mass Data Anal. Images Signals, vol. 9, pp. 3–17,
2018. 3

[50] G. Ganis and S. M. Kosslyn, “Neuroimaging,” in Encyclopedia of the

Human Brain, 2002, pp. 493–505. 3

[51] E. W. Lang, A. M. Tomé, I. R. Keck, J. M. G. Sáez, and C. G. Puntonet,
“Brain connect analysis: A short survey,” Comput. Intell. Neurosci., vol.
2012, pp. 412 512:1–412 512:21, 2012. 3

[52] P. Bellec, C. Chu, F. Chouinard-Decorte, Y. Benhajali, D. S. Margulies,
and R. C. Craddock, “The neuro bureau adhd-200 preprocessed
repository,” NeuroImage, vol. 144, pp. 275–286, 2017. 4

[53] R. C. Petersen, P. Aisen, L. A. Beckett, M. Donohue, A. Gamst, D. J.
Harvey, C. Jack, W. Jagust, L. Shaw, A. Toga et al., “Alzheimer’s disease
neuroimaging initiative (adni): clinical characterization,” Neurology,
vol. 74, pp. 201–209, 2010. 4, 5

[54] D. C. Van Essen, K. Ugurbil, E. Auerbach, D. Barch, T. E. Behrens,
R. Bucholz, A. Chang, L. Chen, M. Corbetta, S. W. Curtiss et al., “The
human connectome project: a data acquisition perspective,” NeuroImage,
vol. 62, pp. 2222–2231, 2012. 4, 5

[55] A. Di Martino, C.-G. Yan, Q. Li, E. Denio, F. X. Castellanos, K. Alaerts,
J. S. Anderson, M. Assaf, S. Y. Bookheimer, M. Dapretto et al., “The
autism brain imaging data exchange: towards a large-scale evaluation
of the intrinsic brain architecture in autism,” Mol. Psychiatry, vol. 19,
pp. 659–667, 2014. 4

[56] Y. Wang, J. Kang, P. B. Kemmer, and Y. Guo, “An efﬁcient and reliable
statistical method for estimating functional connectivity in large scale
brain networks using partial correlation,” Front. Neurosci., vol. 10, p.
123, 2016. 4

[57] S. M. Smith, K. L. Miller, G. Salimi-Khorshidi, M. Webster, C. F.
Beckmann, T. E. Nichols, J. D. Ramsey, and M. W. Woolrich, “Network
modelling methods for fmri,” NeuroImage, vol. 54, pp. 875–891, 2011.
4

[58] K. Marek, D. Jennings, S. Lasch, A. Siderowf, C. Tanner, T. Simuni,
C. Coffey, K. Kieburtz, E. Flagg, S. Chowdhury et al., “The parkinson
progression marker initiative (ppmi),” Prog. Neurobiol., vol. 95, pp.
629–635, 2011. 5

[59] K. A. Ellis, A. I. Bush, D. Darby, D. De Fazio, J. Foster, P. Hudson, N. T.
Lautenschlager, N. Lenzo, R. N. Martins, P. Maruff et al., “The australian
imaging, biomarkers and lifestyle (aibl) study of aging: methodology and
baseline characteristics of 1112 individuals recruited for a longitudinal
study of alzheimer’s disease,” Int Psychogeriatr, vol. 21, pp. 672–687,
2009. 5

[60] A. D. Leow, S. Zhu, L. Zhan, K. McMahon, G. I. de Zubicaray,
M. Meredith, M. Wright, A. Toga, and P. Thompson, “The tensor
distribution function,” Magn Reson Med, vol. 61, pp. 205–214, 2009. 5
[61] T. E. Behrens, M. W. Woolrich, M. Jenkinson, H. Johansen-Berg, R. G.
Nunes, S. Clare, P. M. Matthews, J. M. Brady, and S. M. Smith,
“Characterization and propagation of uncertainty in diffusion-weighted
mr imaging,” Magn Reson Med, vol. 50, pp. 1077–1088, 2003. 5
[62] P. J. Basser, S. Pajevic, C. Pierpaoli, J. Duda, and A. Aldroubi, “In
vivo ﬁber tractography using dt-mri data,” Magn Reson Med, vol. 44,
pp. 625–632, 2000. 5

[63] T. E. Behrens, H. J. Berg, S. Jbabdi, M. F. Rushworth, and M. W.
Woolrich, “Probabilistic diffusion tractography with multiple ﬁbre
orientations: What can we gain?” NeuroImage, vol. 34, pp. 144–155,
2007. 5

[64] L. Zhan, J. Zhou, Y. Wang, Y. Jin, N. Jahanshad, G. Prasad, T. M.
Nir, C. D. Leonardo, J. Ye, P. M. Thompson et al., “Comparison of
nine tractography algorithms for detecting abnormal structural brain
networks in alzheimer’s disease,” Front. Aging Neurosci., vol. 7, p. 48,
2015. 5, 8

[65] G. Deshpande and H. Jia, “Multi-level clustering of dynamic directional
brain network patterns and their behavioral relevance,” Front. Neurosci.,
vol. 13, p. 1448, 2020. 5

[66] Q. Zhu, H. Li, J. Huang, X. Xu, D. Guan, and D. Zhang, “Hybrid
functional brain network with ﬁrst-order and second-order information
for computer-aided diagnosis of schizophrenia,” Front. Neurosci., p.
603, 2019. 5

[67] F. A. Espinoza, V. M. Vergara, E. Damaraju, K. G. Henke, A. Faghiri,
J. A. Turner, A. A. Belger, J. M. Ford, S. C. McEwen, D. H. Mathalon
et al., “Characterizing whole brain temporal variation of functional
connectivity via zero and ﬁrst order derivatives of sliding window

correlations,” Front. Neurosci., vol. 13, p. 634, 2019. 5

[68] L. Kuang, D. Zhao, J. Xing, Z. Chen, F. Xiong, and X. Han, “Metabolic
brain network analysis of fdg-pet in alzheimer’s disease using kernel-
based persistent features,” Molecules, vol. 24, p. 2301, 2019. 5
[69] A. Joudaki, N. Salehi, M. Jalili, and M. G. Knyazeva, “Eeg-based
functional brain networks: does the network size matter?” PLoS One,
vol. 7, p. e35673, 2012. 5

[70] V. D. Calhoun and J. Sui, “Multimodal fusion of brain imaging data:
a key to ﬁnding the missing link (s) in complex mental illness,” Biol
Psychiatry Cogn Neurosci Neuroimaging, vol. 1, pp. 230–244, 2016. 5
[71] H. Cui, Z. Lu, P. Li, and C. Yang, “On positional and structural node
features for graph neural networks on non-attributed graphs,” CIKM,
2022. 6, 10

[72] C. T. Duong, T. D. Hoang, H. T. H. Dang, Q. V. H. Nguyen, and
K. Aberer, “On node features for graph neural networks,” arXiv.org,
2019. 6

[73] F. Errica, M. Podda, D. Bacciu, and A. Micheli, “A fair comparison of
graph neural networks for graph classiﬁcation,” in ICLR, 2020. 6
[74] J. You, R. Ying, and J. Leskovec, “Position-aware graph neural networks,”

in ICML, 2019. 6

[75] Q. Huang, H. He, A. Singh, S. Lim, and A. R. Benson, “Combining label
propagation and simple models out-performs graph neural networks,”
in ICLR, 2021. 6

[76] K. Chaudhuri, F. Chung, and A. Tsiatas, “Spectral clustering of graphs
with general degrees in the extended planted partition model,” in COLT,
2012.

[77] Y. Zhang and K. Rohe, “Understanding regularized spectral clustering

via graph conductance,” in NeurIPS, 2018. 6

[78] C. Cai and Y. Wang, “A Simple Yet Effective Baseline for Non-

Attributed Graph Classiﬁcation,” arXiv.org, 2018. 6

[79] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” NeurIPS, 2017.
7

[80] Z. Niu, G. Zhong, and H. Yu, “A review on the attention mechanism
of deep learning,” Neurocomputing, vol. 452, pp. 48–62, 2021. 7
[81] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
of deep bidirectional transformers for language understanding,” in ACL,
2019. 7

[82] M.-H. Guo, T.-X. Xu, J.-J. Liu, Z.-N. Liu, P.-T. Jiang, T.-J. Mu, S.-
H. Zhang, R. R. Martin, M.-M. Cheng, and S.-M. Hu, “Attention
mechanisms in computer vision: A survey,” arXiv.org, 2021. 7
[83] D. Grattarola, D. Zambon, F. M. Bianchi, and C. Alippi, “Understanding

pooling in graph neural networks,” arXiv.org, 2021. 7

[84] D. Mesquita, A. Souza, and S. Kaski, “Rethinking pooling in graph

neural networks,” NeurIPS, 2020. 7

[85] Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec,
“Hierarchical graph representation learning with differentiable pooling,”
in NeurIPS, 2018. 8

[86] K. Gopinath, C. Desrosiers, and H. Lombaert, “Learnable pooling in
graph convolution networks for brain surface analysis,” IEEE Trans.
Pattern Anal. Mach. Intell., 2020. 8

[87] X. Kan, W. Dai, H. Cui, Z. Zhang, Y. Guo, and Y. Carl, “Brain network

transformer,” in NeurIPS, 2022. 8

[88] A. A. Chen, J. C. Beer, N. J. Tustison, P. A. Cook, R. T. Shinohara,
H. Shou, A. D. N. Initiative et al., “Removal of scanner effects in
covariance improves multivariate pattern analysis in neuroimaging data,”
bioRxiv, p. 858415, 2020. 8

[89] J.-P. Fortin, N. Cullen, Y. I. Sheline, W. D. Taylor, I. Aselcioglu,
P. A. Cook, P. Adams, C. Cooper, M. Fava, P. J. McGrath et al.,
“Harmonization of cortical thickness measurements across scanners and
sites,” NeuroImage, vol. 167, pp. 104–120, 2018. 8

[90] J. Chen, J. Liu, V. D. Calhoun, A. Arias-Vasquez, M. P. Zwiers, C. N.
Gupta, B. Franke, and J. A. Turner, “Exploration of scanning effects in
multi-site structural mri studies,” J. Neurosci. Methods, vol. 230, pp.
37–50, 2014. 8

[91] T. K. Bell, K. J. Godfrey, A. L. Ware, K. O. Yeates, and A. D. Harris,
“Harmonization of multi-site mrs data with combat,” NeuroImage, p.
119330, 2022.

[92] R. Pomponio, G. Erus, M. Habes, J. Doshi, D. Srinivasan, E. Mamourian,
V. Bashyam, I. M. Nasrallah, T. D. Satterthwaite, Y. Fan et al.,
“Harmonization of large mri datasets for the analysis of brain imaging
patterns throughout the lifespan,” NeuroImage, vol. 208, p. 116450,
2020.

[93] A. Yamashita, N. Yahata, T. Itahashi, G. Lisi, T. Yamada, N. Ichikawa,
M. Takamura, Y. Yoshihara, A. Kunimatsu, N. Okada et al., “Harmoniza-
tion of resting-state functional mri data across multiple imaging sites via
the separation of site differences into sampling bias and measurement

14

IEEE TRANSACTIONS ON MEDICAL IMAGING (UNDER REVISION)

bias,” PLOS Biol., vol. 17, p. e3000042, 2019.

[94] M. S. Pinto, R. Paolella, T. Billiet, P. Van Dyck, P.-J. Guns, B. Jeurissen,
A. Ribbens, A. J. den Dekker, and J. Sijbers, “Harmonization of brain
diffusion mri: Concepts and methods,” Front. Neurosci., vol. 14, p. 396,
2020. 8

[95] N. Tzourio-Mazoyer, B. Landeau, D. Papathanassiou, F. Crivello,
O. Etard, N. Delcroix, B. Mazoyer, and M. Joliot, “Automated anatom-
ical labeling of activations in spm using a macroscopic anatomical
parcellation of the mni mri single-subject brain,” NeuroImage, vol. 15,
pp. 273–289, 2002. 8

[96] J. D. Power, A. L. Cohen, S. M. Nelson, G. S. Wig, K. A. Barnes, J. A.
Church, A. C. Vogel, T. O. Laumann, F. M. Miezin, B. L. Schlaggar
et al., “Functional Network Organization of the Human Brain,” Neuron,
vol. 72, pp. 665–678, 2011. 8

[97] S. M. Smith, P. T. Fox, K. L. Miller, D. C. Glahn, P. M. Fox, C. E.
Mackay, N. Filippini, K. E. Watkins, R. Toro, A. R. Laird et al.,
“Correspondence of the brain’s functional architecture during activation
and rest,” Proc. Natl. Acad. Sci. U.S.A., vol. 106, pp. 13 040–13 045,
2009. 8

[98] R. S. Desikan, F. Ségonne, B. Fischl, B. T. Quinn, B. C. Dickerson,
D. Blacker, R. L. Buckner, A. M. Dale, R. P. Maguire, B. T. Hyman
et al., “An automated labeling system for subdividing the human cerebral
cortex on mri scans into gyral based regions of interest,” NeuroImage,
vol. 31, pp. 968–980, 2006. 8

[99] B. Casey, T. Cannonier, M. I. Conley, A. O. Cohen, D. M. Barch, M. M.
Heitzeg, M. E. Soules, T. Teslovich, D. V. Dellarco, H. Garavan et al.,
“The adolescent brain cognitive development (abcd) study: imaging
acquisition across 21 sites,” Dev Cogn Neurosci, vol. 32, pp. 43–54,
2018. 9

[100] M. F. Glasser, S. N. Sotiropoulos, J. A. Wilson, T. S. Coalson, B. Fischl,
J. L. Andersson, J. Xu, S. Jbabdi, M. Webster, J. R. Polimeni, D. C.
Van Essen, and M. Jenkinson, “The minimal preprocessing pipelines
for the human connectome project,” NeuroImage, vol. 80, pp. 105–124,
2013. 9

[101] H. Lu et al., “Mpca: Multilinear principal component analysis of tensor

objects,” IEEE Trans. Neural Netw., 2008. 9

[102] M. Dyrba et al., “Multimodal analysis of functional and structural
disconnection in alzheimer’s disease using multiple kernel svm,” Hum
Brain Mapp, 2015. 9

[103] F. Nie, Z. Zeng, I. W. Tsang, D. Xu, and C. Zhang, “Spectral embedded
clustering: A framework for in-sample and out-of-sample spectral
clustering,” IEEE Trans. Neural Netw., vol. 22, pp. 1796–1808, 2011. 9
[104] F. Nie, J. Li, X. Li et al., “Parameter-free auto-weighted multiple graph
learning: a framework for multiview clustering and semi-supervised
classiﬁcation.” in IJCAI, 2016. 9

[105] A. Paszke, S. Gross et al., “PyTorch: an imperative style, high-

performance deep learning library,” in NeurIPS, 2019. 9

[106] M. Fey and J. E. Lenssen, “Fast graph representation learning with

PyTorch Geometric,” in RLGM@ICLR, 2019. 10

[107] Y. Yang, Y. Zhu, H. Cui, X. Kan, L. He, Y. Guo, and C. Yang, “Data-
efﬁcient brain connectome analysis via multi-task meta-learning,” KDD,
2022. 11


